{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":120005,"sourceType":"modelInstanceVersion","isSourceIdPinned":true,"modelInstanceId":100936,"modelId":121027}],"dockerImageVersionId":30787,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:39:40.189339Z","iopub.execute_input":"2024-11-06T13:39:40.189725Z","iopub.status.idle":"2024-11-06T13:39:42.223795Z","shell.execute_reply.started":"2024-11-06T13:39:40.189683Z","shell.execute_reply":"2024-11-06T13:39:42.222792Z"}},"outputs":[{"name":"stdout","text":"/kaggle/input/llama-3.2/transformers/3b-instruct/1/model.safetensors.index.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/config.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model-00001-of-00002.safetensors\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/model-00002-of-00002.safetensors\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/README.md\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/USE_POLICY.md\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/tokenizer.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/tokenizer_config.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/LICENSE.txt\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/special_tokens_map.json\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/.gitattributes\n/kaggle/input/llama-3.2/transformers/3b-instruct/1/generation_config.json\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"%%capture\n%pip install -U transformers \n%pip install -U datasets \n%pip install -U accelerate \n%pip install -U peft \n%pip install -U trl \n%pip install -U bitsandbytes \n%pip install -U wandb","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:39:42.225549Z","iopub.execute_input":"2024-11-06T13:39:42.225971Z","iopub.status.idle":"2024-11-06T13:42:08.578994Z","shell.execute_reply.started":"2024-11-06T13:39:42.225935Z","shell.execute_reply":"2024-11-06T13:42:08.577626Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"from transformers import (\n    AutoModelForCausalLM,\n    AutoTokenizer,\n    BitsAndBytesConfig,\n    HfArgumentParser,\n    TrainingArguments,\n    pipeline,\n    logging,\n)\nfrom peft import (\n    LoraConfig,\n    PeftModel,\n    prepare_model_for_kbit_training,\n    get_peft_model,\n)\nimport os, torch, wandb\nfrom datasets import load_dataset\nfrom trl import SFTTrainer, setup_chat_format","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:42:08.581005Z","iopub.execute_input":"2024-11-06T13:42:08.581331Z","iopub.status.idle":"2024-11-06T13:42:40.788495Z","shell.execute_reply.started":"2024-11-06T13:42:08.581297Z","shell.execute_reply":"2024-11-06T13:42:40.787576Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"from huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_TOKEN\")\nlogin(token = hf_token)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:45:50.341793Z","iopub.execute_input":"2024-11-06T13:45:50.342237Z","iopub.status.idle":"2024-11-06T13:45:50.593538Z","shell.execute_reply.started":"2024-11-06T13:45:50.342194Z","shell.execute_reply":"2024-11-06T13:45:50.592573Z"}},"outputs":[{"name":"stdout","text":"The token has not been saved to the git credentials helper. Pass `add_to_git_credential=True` in this function directly or `--add-to-git-credential` if using via `huggingface-cli` if you want to set the git credential as well.\nToken is valid (permission: write).\nYour token has been saved to /root/.cache/huggingface/token\nLogin successful\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"wb_token = user_secrets.get_secret(\"wandb\")\n\nwandb.login(key=wb_token)\nrun = wandb.init(\n    project='Fine-tune Llama 3.2 on Finance Dataset', \n    job_type=\"training\", \n    anonymous=\"allow\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:45:53.059116Z","iopub.execute_input":"2024-11-06T13:45:53.059517Z","iopub.status.idle":"2024-11-06T13:45:56.523140Z","shell.execute_reply.started":"2024-11-06T13:45:53.059474Z","shell.execute_reply":"2024-11-06T13:45:56.522160Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend. Please refer to https://wandb.me/wandb-core for more information.\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmed-houbid\u001b[0m (\u001b[33mmed-houbid-enset-mohammedia\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.18.5"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20241106_134554-kkp68auq</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset/runs/kkp68auq' target=\"_blank\">light-plant-2</a></strong> to <a href='https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset' target=\"_blank\">https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset/runs/kkp68auq' target=\"_blank\">https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset/runs/kkp68auq</a>"},"metadata":{}}],"execution_count":9},{"cell_type":"code","source":"base_model = \"/kaggle/input/llama-3.2/transformers/3b-instruct/1\"\nnew_model = \"llama-3.2-3b-Finance-Consultant-ChatBot\"\ndataset_name = \"gbharti/finance-alpaca\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:45:59.335681Z","iopub.execute_input":"2024-11-06T13:45:59.336045Z","iopub.status.idle":"2024-11-06T13:45:59.341998Z","shell.execute_reply.started":"2024-11-06T13:45:59.336012Z","shell.execute_reply":"2024-11-06T13:45:59.341086Z"}},"outputs":[],"execution_count":10},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# Set torch dtype and attention implementation\nif torch.cuda.get_device_capability()[0] >= 8:\n    !pip install -qqq flash-attn\n    torch_dtype = torch.bfloat16\n    attn_implementation = \"flash_attention_2\"\nelse:\n    torch_dtype = torch.float16\n    attn_implementation = \"eager\"","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:46:01.247054Z","iopub.execute_input":"2024-11-06T13:46:01.247428Z","iopub.status.idle":"2024-11-06T13:46:01.332072Z","shell.execute_reply.started":"2024-11-06T13:46:01.247392Z","shell.execute_reply":"2024-11-06T13:46:01.331285Z"}},"outputs":[],"execution_count":11},{"cell_type":"code","source":"# QLoRA config\nbnb_config = BitsAndBytesConfig(\n    load_in_4bit=True,\n    bnb_4bit_quant_type=\"nf4\",\n    bnb_4bit_compute_dtype=torch_dtype,\n    bnb_4bit_use_double_quant=True,\n)\n# Load model\nmodel = AutoModelForCausalLM.from_pretrained(\n    base_model,\n    quantization_config=bnb_config,\n    device_map=\"auto\",\n    attn_implementation=attn_implementation\n)\n\n# Load tokenizer\ntokenizer = AutoTokenizer.from_pretrained(base_model, trust_remote_code=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:46:02.598323Z","iopub.execute_input":"2024-11-06T13:46:02.599023Z","iopub.status.idle":"2024-11-06T13:46:39.705613Z","shell.execute_reply.started":"2024-11-06T13:46:02.598981Z","shell.execute_reply":"2024-11-06T13:46:39.704726Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5cd99fa096664584aeb91d4b8e73227a"}},"metadata":{}}],"execution_count":12},{"cell_type":"code","source":"dataset = load_dataset(dataset_name, split=\"train\")\ndataset = dataset.shuffle(seed=65).select(range(1000)) # Only use 1000 samples for quick demo\ninstruction = \"\"\"You are a top-rated finance consultant named Med.\n    Be polite to clients and provide clear, accurate advice on financial matters, including investments, savings, and financial planning.\n    Always ensure your responses are professional and tailored to the client's financial needs.\n    \"\"\"\ndef format_chat_template(row):\n    \n    row_json = [\n        {\"role\": \"system\", \"content\": instruction},\n        {\"role\": \"user\", \"content\": row[\"instruction\"]},\n        {\"role\": \"assistant\", \"content\": row[\"output\"]}  # Correct column name for the assistant's response\n    ]\n    \n    row[\"text\"] = tokenizer.apply_chat_template(row_json, tokenize=False)\n    return row\n\ndataset = dataset.map(\n    format_chat_template,\n    num_proc= 4,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:46:39.734556Z","iopub.execute_input":"2024-11-06T13:46:39.734815Z","iopub.status.idle":"2024-11-06T13:46:42.940174Z","shell.execute_reply.started":"2024-11-06T13:46:39.734785Z","shell.execute_reply":"2024-11-06T13:46:42.939128Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"README.md:   0%|          | 0.00/709 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"254e3dd0b1434001a39b1e23107324a0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Cleaned_date.json:   0%|          | 0.00/42.9M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"647b4be9cadc4b908330f10a3d9568b8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/68912 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ceff30423cdb4d86a259e2ad52027e5e"}},"metadata":{}},{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/multiprocess/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n  self.pid = os.fork()\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map (num_proc=4):   0%|          | 0/1000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ddcef1760be84962b0768ec99e99ddc1"}},"metadata":{}}],"execution_count":13},{"cell_type":"code","source":"dataset['text'][3]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:47:01.750102Z","iopub.execute_input":"2024-11-06T13:47:01.750510Z","iopub.status.idle":"2024-11-06T13:47:01.761401Z","shell.execute_reply.started":"2024-11-06T13:47:01.750465Z","shell.execute_reply":"2024-11-06T13:47:01.760566Z"}},"outputs":[{"execution_count":14,"output_type":"execute_result","data":{"text/plain":"'<|begin_of_text|><|start_header_id|>system<|end_header_id|>\\n\\nYou are a top-rated finance consultant named Med.\\n    Be polite to clients and provide clear, accurate advice on financial matters, including investments, savings, and financial planning.\\n    Always ensure your responses are professional and tailored to the client\\'s financial needs.<|eot_id|><|start_header_id|>user<|end_header_id|>\\n\\nFind an article about the Covid-19 vaccine.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\nThe article I found is titled \"A Vaccine for COVID-19 Is On the Way\" from The New York Times. It explains the progress that has be made in developing a vaccine and what challenges remain. The article also addresses the difficulties of obtaining regulatory approval, the different approaches scientists have taken to create vaccines, the potential timeline for immunization, and the steps that are being taken to ensure that the vaccine is safe.<|eot_id|><|start_header_id|>assistant<|end_header_id|>\\n\\n'"},"metadata":{}}],"execution_count":14},{"cell_type":"code","source":"import bitsandbytes as bnb\n\ndef find_all_linear_names(model):\n    cls = bnb.nn.Linear4bit\n    lora_module_names = set()\n    for name, module in model.named_modules():\n        if isinstance(module, cls):\n            names = name.split('.')\n            lora_module_names.add(names[0] if len(names) == 1 else names[-1])\n    if 'lm_head' in lora_module_names:  # needed for 16 bit\n        lora_module_names.remove('lm_head')\n    return list(lora_module_names)\n\nmodules = find_all_linear_names(model)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:47:04.648845Z","iopub.execute_input":"2024-11-06T13:47:04.649654Z","iopub.status.idle":"2024-11-06T13:47:04.659339Z","shell.execute_reply.started":"2024-11-06T13:47:04.649606Z","shell.execute_reply":"2024-11-06T13:47:04.658316Z"}},"outputs":[],"execution_count":15},{"cell_type":"code","source":"# LoRA config\npeft_config = LoraConfig(\n    r=16,\n    lora_alpha=32,\n    lora_dropout=0.05,\n    bias=\"none\",\n    task_type=\"CAUSAL_LM\",\n    target_modules=modules\n)\nmodel, tokenizer = setup_chat_format(model, tokenizer)\nmodel = get_peft_model(model, peft_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:47:06.824463Z","iopub.execute_input":"2024-11-06T13:47:06.824866Z","iopub.status.idle":"2024-11-06T13:47:06.970742Z","shell.execute_reply.started":"2024-11-06T13:47:06.824830Z","shell.execute_reply":"2024-11-06T13:47:06.969494Z"}},"outputs":[{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)","Cell \u001b[0;32mIn[16], line 10\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# LoRA config\u001b[39;00m\n\u001b[1;32m      2\u001b[0m peft_config \u001b[38;5;241m=\u001b[39m LoraConfig(\n\u001b[1;32m      3\u001b[0m     r\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m      4\u001b[0m     lora_alpha\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m32\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      8\u001b[0m     target_modules\u001b[38;5;241m=\u001b[39mmodules\n\u001b[1;32m      9\u001b[0m )\n\u001b[0;32m---> 10\u001b[0m model, tokenizer \u001b[38;5;241m=\u001b[39m \u001b[43msetup_chat_format\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtokenizer\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m model \u001b[38;5;241m=\u001b[39m get_peft_model(model, peft_config)\n","File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/trl/models/utils.py:101\u001b[0m, in \u001b[0;36msetup_chat_format\u001b[0;34m(model, tokenizer, format, resize_to_multiple_of)\u001b[0m\n\u001b[1;32m     99\u001b[0m \u001b[38;5;66;03m# check if model already had a chat template\u001b[39;00m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m tokenizer\u001b[38;5;241m.\u001b[39mchat_template \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 101\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    102\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mChat template is already added to the tokenizer. If you want to overwrite it, please set it to None\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    103\u001b[0m     )\n\u001b[1;32m    105\u001b[0m \u001b[38;5;66;03m# check if format available and retrieve\u001b[39;00m\n\u001b[1;32m    106\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mformat\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m FORMAT_MAPPING:\n","\u001b[0;31mValueError\u001b[0m: Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None"],"ename":"ValueError","evalue":"Chat template is already added to the tokenizer. If you want to overwrite it, please set it to None","output_type":"error"}],"execution_count":16},{"cell_type":"code","source":"#Hyperparamter\ntraining_arguments = TrainingArguments(\n    output_dir=new_model,\n    per_device_train_batch_size=1,\n    per_device_eval_batch_size=1,\n    gradient_accumulation_steps=2,\n    optim=\"paged_adamw_32bit\",\n    num_train_epochs=1,\n    eval_strategy=\"steps\",\n    eval_steps=0.2,\n    logging_steps=1,\n    warmup_steps=10,\n    logging_strategy=\"steps\",\n    learning_rate=2e-4,\n    fp16=False,\n    bf16=False,\n    group_by_length=True,\n    report_to=\"wandb\"\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:47:14.414057Z","iopub.execute_input":"2024-11-06T13:47:14.414464Z","iopub.status.idle":"2024-11-06T13:47:14.450705Z","shell.execute_reply.started":"2024-11-06T13:47:14.414424Z","shell.execute_reply":"2024-11-06T13:47:14.449826Z"}},"outputs":[],"execution_count":17},{"cell_type":"code","source":"train_dataset, test_dataset = dataset.train_test_split(test_size=0.2).values()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:48:43.581553Z","iopub.execute_input":"2024-11-06T13:48:43.582438Z","iopub.status.idle":"2024-11-06T13:48:43.599561Z","shell.execute_reply.started":"2024-11-06T13:48:43.582397Z","shell.execute_reply":"2024-11-06T13:48:43.598705Z"}},"outputs":[],"execution_count":19},{"cell_type":"code","source":"# Setting sft parameters\ntrainer = SFTTrainer(\n    model=model,\n    train_dataset=train_dataset,\n    eval_dataset=test_dataset,\n    peft_config=peft_config,\n    max_seq_length=512,\n    dataset_text_field=\"text\",\n    tokenizer=tokenizer,\n    args=training_arguments,\n    packing=False,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:49:03.545277Z","iopub.execute_input":"2024-11-06T13:49:03.545677Z","iopub.status.idle":"2024-11-06T13:49:05.609040Z","shell.execute_reply.started":"2024-11-06T13:49:03.545638Z","shell.execute_reply":"2024-11-06T13:49:05.608020Z"}},"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.10/site-packages/huggingface_hub/utils/_deprecation.py:100: FutureWarning: Deprecated argument(s) used in '__init__': max_seq_length, dataset_text_field. Will not be supported from version '0.13.0'.\n\nDeprecated positional argument(s) used in SFTTrainer, please use the SFTConfig to set these arguments instead.\n  warnings.warn(message, FutureWarning)\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:300: UserWarning: You passed a `max_seq_length` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n/opt/conda/lib/python3.10/site-packages/trl/trainer/sft_trainer.py:328: UserWarning: You passed a `dataset_text_field` argument to the SFTTrainer, the value you passed will override the one in the `SFTConfig`.\n  warnings.warn(\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/800 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"12540190d708417cb5abb73996583eac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/200 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5fb5f660d3aa467c83715e24fba95873"}},"metadata":{}}],"execution_count":20},{"cell_type":"code","source":"if tokenizer.pad_token is None:\n    tokenizer.pad_token = tokenizer.eos_token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:51:00.325375Z","iopub.execute_input":"2024-11-06T13:51:00.325812Z","iopub.status.idle":"2024-11-06T13:51:00.331394Z","shell.execute_reply.started":"2024-11-06T13:51:00.325773Z","shell.execute_reply":"2024-11-06T13:51:00.330481Z"}},"outputs":[],"execution_count":22},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T13:51:03.467974Z","iopub.execute_input":"2024-11-06T13:51:03.468858Z","iopub.status.idle":"2024-11-06T14:13:42.980208Z","shell.execute_reply.started":"2024-11-06T13:51:03.468815Z","shell.execute_reply":"2024-11-06T14:13:42.979386Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='400' max='400' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [400/400 22:33, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n      <th>Validation Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>80</td>\n      <td>0.867700</td>\n      <td>1.024872</td>\n    </tr>\n    <tr>\n      <td>160</td>\n      <td>1.764500</td>\n      <td>1.018865</td>\n    </tr>\n    <tr>\n      <td>240</td>\n      <td>0.529700</td>\n      <td>0.997875</td>\n    </tr>\n    <tr>\n      <td>320</td>\n      <td>0.819100</td>\n      <td>0.991814</td>\n    </tr>\n    <tr>\n      <td>400</td>\n      <td>0.264100</td>\n      <td>0.989076</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stderr","text":"Trainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\nTrainer.tokenizer is now deprecated. You should use Trainer.processing_class instead.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"TrainOutput(global_step=400, training_loss=1.0699986746162176, metrics={'train_runtime': 1358.8747, 'train_samples_per_second': 0.589, 'train_steps_per_second': 0.294, 'total_flos': 2343035007946752.0, 'train_loss': 1.0699986746162176, 'epoch': 1.0})"},"metadata":{}}],"execution_count":23},{"cell_type":"code","source":"wandb.finish()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T14:14:06.126891Z","iopub.execute_input":"2024-11-06T14:14:06.127298Z","iopub.status.idle":"2024-11-06T14:14:07.456303Z","shell.execute_reply.started":"2024-11-06T14:14:06.127260Z","shell.execute_reply":"2024-11-06T14:14:07.455380Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n    </style>\n<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>█▇▃▂▁</td></tr><tr><td>eval/runtime</td><td>▁▄▃▆█</td></tr><tr><td>eval/samples_per_second</td><td>█▃▆▃▁</td></tr><tr><td>eval/steps_per_second</td><td>█▃▆▃▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▂▂▃▃▃▃▃▃▄▄▄▅▅▅▅▅▆▆▆▆▆▇▇▇▇▇▇▇▇▇▇█████</td></tr><tr><td>train/global_step</td><td>▁▁▁▁▂▂▂▂▂▂▃▃▃▃▃▃▄▄▄▄▄▄▅▅▅▅▅▅▅▆▆▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>█▄▅▅▆▃▆▄▄▄▅▅▂▃▃▃▄▅▂▃▂▂▃▅▅▃▅▄▂▂▄▃▃▄▄▂▂▂▅▁</td></tr><tr><td>train/learning_rate</td><td>▇████▇▇▇▆▆▅▅▅▅▅▄▄▄▄▄▄▄▄▃▃▃▃▃▃▃▂▂▂▂▂▁▁▁▁▁</td></tr><tr><td>train/loss</td><td>█▇▅▄▂▆▆▃▁▇▁▅▄▅▄▂▁▁▁▁▆▂▂▃▂▄▄▂▃▅▅▃▂▁▂▇▃▃▂▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/loss</td><td>0.98908</td></tr><tr><td>eval/runtime</td><td>103.321</td></tr><tr><td>eval/samples_per_second</td><td>1.936</td></tr><tr><td>eval/steps_per_second</td><td>1.936</td></tr><tr><td>total_flos</td><td>2343035007946752.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>400</td></tr><tr><td>train/grad_norm</td><td>0.49768</td></tr><tr><td>train/learning_rate</td><td>0</td></tr><tr><td>train/loss</td><td>0.2641</td></tr><tr><td>train_loss</td><td>1.07</td></tr><tr><td>train_runtime</td><td>1358.8747</td></tr><tr><td>train_samples_per_second</td><td>0.589</td></tr><tr><td>train_steps_per_second</td><td>0.294</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">light-plant-2</strong> at: <a href='https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset/runs/kkp68auq' target=\"_blank\">https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset/runs/kkp68auq</a><br/> View project at: <a href='https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset' target=\"_blank\">https://wandb.ai/med-houbid-enset-mohammedia/Fine-tune%20Llama%203.2%20on%20Finance%20Dataset</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20241106_134554-kkp68auq/logs</code>"},"metadata":{}}],"execution_count":24},{"cell_type":"code","source":"# Define the test question message\nmessages = [\n    {\"role\": \"system\", \"content\": instruction},\n    {\"role\": \"user\", \"content\": \"How can I save on closing costs when buying a home?\"}\n]\n\n# Apply the chat template to format the input properly\nprompt = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n\n# Tokenize the input\ninputs = tokenizer(prompt, return_tensors='pt', padding=True, truncation=True).to(\"cuda\")\n\n# Generate a response from the model\noutputs = model.generate(**inputs, max_new_tokens=150, num_return_sequences=1)\n\n# Decode the generated response\ntext = tokenizer.decode(outputs[0], skip_special_tokens=True)\n\n# Extract and print the response\nprint(text.split(\"assistant\")[1])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T14:16:55.226127Z","iopub.execute_input":"2024-11-06T14:16:55.226672Z","iopub.status.idle":"2024-11-06T14:17:15.559856Z","shell.execute_reply.started":"2024-11-06T14:16:55.226605Z","shell.execute_reply":"2024-11-06T14:17:15.558796Z"}},"outputs":[{"name":"stderr","text":"Setting `pad_token_id` to `eos_token_id`:None for open-end generation.\n","output_type":"stream"},{"name":"stdout","text":"\n\nThere are several ways to save on closing costs when buying a home. One way is to negotiate with the seller to pay some of the closing costs. Another way is to consider a home with a lower purchase price, which will result in lower closing costs. Additionally, you can consider a mortgage that has lower closing costs, such as an adjustable-rate mortgage. You can also consider using a mortgage broker to help you negotiate with the seller and to find the best mortgage options for you. Another option is to consider a home with a lower purchase price and to use a mortgage broker to help you negotiate with the seller and to find the best mortgage options for you. Finally, you can also consider using a closing cost calculator to help you estimate your closing costs and\n","output_type":"stream"}],"execution_count":25},{"cell_type":"code","source":"trainer.model.save_pretrained(new_model)\ntrainer.model.push_to_hub(new_model, use_temp_dir=False)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2024-11-06T14:19:24.134516Z","iopub.execute_input":"2024-11-06T14:19:24.134947Z","iopub.status.idle":"2024-11-06T14:19:29.709757Z","shell.execute_reply.started":"2024-11-06T14:19:24.134913Z","shell.execute_reply":"2024-11-06T14:19:29.708754Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/97.3M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"af496be45a18454884e9532665a58c0b"}},"metadata":{}},{"execution_count":26,"output_type":"execute_result","data":{"text/plain":"CommitInfo(commit_url='https://huggingface.co/Houbid/llama-3.2-3b-Finance-Consultant-ChatBot/commit/a80c61d7216bb529c23c6dd04feda1608dfa84f0', commit_message='Upload model', commit_description='', oid='a80c61d7216bb529c23c6dd04feda1608dfa84f0', pr_url=None, repo_url=RepoUrl('https://huggingface.co/Houbid/llama-3.2-3b-Finance-Consultant-ChatBot', endpoint='https://huggingface.co', repo_type='model', repo_id='Houbid/llama-3.2-3b-Finance-Consultant-ChatBot'), pr_revision=None, pr_num=None)"},"metadata":{}}],"execution_count":26},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}